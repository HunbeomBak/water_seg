{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c508bf9f-42c7-4299-96e9-b2c60fc7f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danusys16\\anaconda3\\envs\\yolov8\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "C:\\Users\\danusys16\\anaconda3\\envs\\yolov8\\lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "D:\\015_segmentation\\Custom_SAM\\Module\\sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with Module.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "D:\\015_segmentation\\Custom_SAM\\Module\\sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with Module.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "D:\\015_segmentation\\Custom_SAM\\Module\\sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with Module.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "D:\\015_segmentation\\Custom_SAM\\Module\\sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with Module.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "D:\\015_segmentation\\Custom_SAM\\Module\\sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with Module.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from Module.sam import sam_model_registry, SamPredictor\n",
    "from Module.sam.utils.transforms import ResizeLongestSide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a50011-1bf8-4c9b-8e80-87eea100e832",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2fcf22-b1ba-43c7-91a2-18b9ac0ee916",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'vit_t'\n",
    "checkpoint = \"../runs/241119_vit-b_to_vit-t_DANU_WS_v2/tiny_vit_best.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0d03ff2-d976-4a2c-925b-823aaa3d4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.split(checkpoint)[0]\n",
    "\n",
    "output_name = os.path.split(checkpoint)[1]\n",
    "output_name = os.path.splitext(output_name)[0]+\"_encoder.onnx\" \n",
    "\n",
    "output_path = os.path.join(save_dir, output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31eb5711-da66-44ff-a7be-e151b92195c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "transform = ResizeLongestSide(sam.image_encoder.img_size)\n",
    "\n",
    "pixel_mean = torch.Tensor([123.675, 116.28, 103.53]).view(-1, 1, 1)\n",
    "pixel_std = torch.Tensor([58.395, 57.12, 57.375]).view(-1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c159ba6-2fe0-4859-bb6b-1a4e42098aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = np.zeros((sam.image_encoder.img_size, sam.image_encoder.img_size, 3), dtype=np.uint8)\n",
    "dummy_input = transform.apply_image(dummy_input)\n",
    "dummy_input = torch.as_tensor(dummy_input, device='cpu')\n",
    "dummy_input = dummy_input.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "#dummy_input = (dummy_input - pixel_mean) / pixel_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b45e7e2-f646-41b7-8dfd-175dc48ce133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024, 1024])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db16e520-4857-4eb0-8d92-09bcf5dc352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_names = ['image_embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "253d018a-7472-401d-8fab-b3bac2d4f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, image_size, checkpoint, model_type):\n",
    "        super().__init__()\n",
    "        self.sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "        self.sam.to(device='cpu')\n",
    "        self.predictor = SamPredictor(self.sam)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.predictor.set_torch_image(x, (self.image_size))\n",
    "        return self.predictor.get_image_embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd6bdf4a-0ca9-4608-9c55-40b6b59e3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_model =Model(image_size=(1024,1024),\n",
    "                checkpoint=checkpoint,\n",
    "                model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b32951ba-3e30-4e59-9c3a-e40fc234e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\015_segmentation\\Custom_SAM\\Module\\sam\\predictor.py:81: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and transformed_image.shape[1] == 3\n",
      "D:\\015_segmentation\\Custom_SAM\\Module\\sam\\predictor.py:82: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and max(*transformed_image.shape[2:]) == self.model.image_encoder.img_size\n",
      "D:\\015_segmentation\\Custom_SAM\\Module\\sam\\modeling\\tiny_vit_sam.py:338: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert L == H * W, \"input feature has wrong size\"\n",
      "D:\\015_segmentation\\Custom_SAM\\Module\\sam\\modeling\\tiny_vit_sam.py:136: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  B = len(x)\n"
     ]
    }
   ],
   "source": [
    "model_trace = torch.jit.trace(sam_model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "583e3cc3-6d79-4691-b57c-a04cdbf3afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danusys16\\anaconda3\\envs\\yolov8\\lib\\site-packages\\torch\\onnx\\utils.py:847: UserWarning: no signature found for <torch.ScriptMethod object at 0x0000025ECDC419F0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n",
      "C:\\Users\\danusys16\\anaconda3\\envs\\yolov8\\lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "C:\\Users\\danusys16\\anaconda3\\envs\\yolov8\\lib\\site-packages\\torch\\onnx\\utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "C:\\Users\\danusys16\\anaconda3\\envs\\yolov8\\lib\\site-packages\\torch\\onnx\\utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model_trace, \n",
    "                  dummy_input, \n",
    "                  output_path,\n",
    "                  input_names=['input'], \n",
    "                  output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4c7ec4e-78e6-4c36-9ea1-e340af75f24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../runs/241119_vit-b_to_vit-t_DANU_WS_v2\\\\tiny_vit_best_encoder.onnx'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111378b-b2f0-433f-9897-7f44c5228854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4ac7a-550c-43ff-9e89-87226dc0aaba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
