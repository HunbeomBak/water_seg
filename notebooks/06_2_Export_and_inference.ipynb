{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496fa459-2822-4888-8a41-70ff0a19f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c03cf8-c31d-451d-9768-6b6889cd1e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danusys\\anaconda3\\envs\\nanosam\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\danusys\\anaconda3\\envs\\nanosam\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "C:\\Users\\danusys\\anaconda3\\envs\\nanosam\\lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "d:\\watersegmentation\\mobilesam\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "d:\\watersegmentation\\mobilesam\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "d:\\watersegmentation\\mobilesam\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "d:\\watersegmentation\\mobilesam\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "d:\\watersegmentation\\mobilesam\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch2trt import torch2trt\n",
    "import warnings\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "\n",
    "from Module.models.timm_image_encoder import TimmImageEncoder\n",
    "from mobile_sam.modeling.sam import Sam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a3b47-1c2a-48e4-8af4-fad6eb145d7f",
   "metadata": {},
   "source": [
    "## Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffb673-7936-4d3e-a3a5-4025ce0ba8a4",
   "metadata": {},
   "source": [
    "### Common Settomg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7ee0ba-1e2f-4054-b2eb-b6498b328d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "opset=16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002dfd5-28e8-422b-8aee-641dbc8f2859",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b3d0b6-3458-4e87-82b1-420d6ee0ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_type = 'resnet18'\n",
    "enc_checkpoint = \"./runs/241031_vit-b_to_resnet18_v2/Nanosam_encoder.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae54113c-e254-40b7-af37-f68d4c27ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model load\n",
    "sam_enc = TimmImageEncoder('resnet18', pretrained=True)\n",
    "sam_enc.load_state_dict(torch.load(enc_checkpoint)[\"model\"])\n",
    "\n",
    "sam_enc.to(device)\n",
    "sam_enc.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c773bf-674a-4da7-934e-450f3411bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=1024\n",
    "dummy_input = torch.randn(1, 3, input_size, input_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85034731-cb6d-4551-b90e-8be9a78cbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_trt = torch2trt(sam_enc, [dummy_input], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c37bfac-5235-4bf5-90f6-556eedc42fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_enc_path = enc_checkpoint.replace(\".pth\", \".onnx\")\n",
    "\n",
    "torch.onnx.export(\n",
    "            sam_enc,\n",
    "            (dummy_input,),\n",
    "            output_enc_path,\n",
    "            input_names=[\"image\"],\n",
    "            output_names=[\"image_embeddings\"],\n",
    "            opset_version=opset\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b554d6-4787-42ed-954b-1cef6505d8d9",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4bf182-0d41-4a91-8086-f01b758b2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_model_type = 'vit_b'\n",
    "dec_checkpoint = '../runs/241115_SAM_ViT_b_dataset_V2_ft_v1/best.pth'\n",
    "\n",
    "sam = sam_model_registry[dec_model_type](checkpoint=dec_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074d736e-25da-4e15-980e-3f821a60c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = SamOnnxModel(\n",
    "    model=sam, \n",
    "    return_single_mask=True,\n",
    "    use_stability_score=False,\n",
    "    return_extra_metrics=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc2a02b9-a46e-4a02-a5fb-caf1bea0a50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../runs/241115_SAM_ViT_b_dataset_V2_ft_v1\\\\best_decoder.onnx'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = os.path.split(dec_checkpoint)[0]\n",
    "\n",
    "output_name = os.path.split(dec_checkpoint)[1]\n",
    "output_name = os.path.splitext(output_name)[0]+\"_decoder.onnx\" \n",
    "\n",
    "output_dec_path = os.path.join(save_dir, output_name)\n",
    "output_dec_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e2ddfb6-f044-4c12-a1a1-218b7e6223c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_axes = {\n",
    "        \"point_coords\": {1: \"num_points\"},\n",
    "        \"point_labels\": {1: \"num_points\"},\n",
    "    }\n",
    "\n",
    "embed_dim = sam.prompt_encoder.embed_dim\n",
    "embed_size = sam.prompt_encoder.image_embedding_size\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "dummy_inputs = {\n",
    "        \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
    "        \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
    "        \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
    "        \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "        \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
    "        \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.int32),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "423553a5-21aa-4b9a-915f-a0e44ccfe6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = onnx_model(**dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eefa0aff-cf51-4c84-9fd2-c52210a13c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c118e38-4a3c-4ebc-9a1a-583d603215fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting onnx model to ../runs/241115_SAM_ViT_b_dataset_V2_ft_v1\\best_decoder.onnx...\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    with open(output_dec_path, \"wb\") as f:\n",
    "        print(f\"Exporting onnx model to {output_dec_path}...\")\n",
    "        torch.onnx.export(\n",
    "                onnx_model,\n",
    "                tuple(dummy_inputs.values()),\n",
    "                f,\n",
    "                export_params=True,\n",
    "                verbose=False,\n",
    "                opset_version=opset,\n",
    "                do_constant_folding=True,\n",
    "                input_names=list(dummy_inputs.keys()),\n",
    "                output_names=output_names,\n",
    "                dynamic_axes=dynamic_axes,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0a947-7dfd-4b63-aede-57046b7569b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
